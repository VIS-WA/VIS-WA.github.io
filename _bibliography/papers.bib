---
---

@misc{tinyllm,
      title={TinyLLM: A Framework for Training and Deploying Language Models at the Edge Computers}, 
      author={Savitha Viswanadh Kandala and Pramuka Medaranga and Ambuj Varshney},
      year={2024},
      eprint={2412.15304},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2412.15304},
      bibtex_show = {true},
      website={https://tinyllm.org},
      selected={true}, 
}

@inproceedings{s3,
  author = {Kandala, Savitha Viswanadh and Varshney, Ambuj},
  title = {A Framework for Training and Deploying Foundational Language Models for Embedded Sensing},
  year = {2024},
  isbn = {9798400704895},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3636534.3695901},
  doi = {10.1145/3636534.3695901},
  abstract = {Large language models have attracted a significant recent interest, with an overwhelming efforts and emphasis on scaling their parameter size to support general-purpose and emergent capabilities. However, memory and processing requirements for model inference also scales proportionally with the model's parameter size. This makes it challenging for state-of-the-art, larger models to perform inference locally on edge and mobile devices, even though such models could benefit numerous tasks on these systems. Consequently, these devices are often restricted to accessing larger models through network calls. This approach introduces challenges related to increased inference latency due to network delays and provider capacity. It also raises concerns about sharing private information with third-party vendors. To address these issues, we are developing a system called OTTER. This system tackles the particular problem by enabling the training of smaller yet highly capable foundational language models. As a result of the reduced parameter size, these models can run locally even on constrained edge devices, such as mobile phones and wearables, and are able to provide low-latency responses compared with their larger remotely hosted counterparts. We present our ongoing work describing the framework as applied to training a smaller foundational model for embedded sensing application for tracking a person's breathing with high accuracy comparable to models orders of magnitude larger in size. Our results demonstrate that carefully pre-trained and fine-tuned smaller sized models outperform much larger counterparts for some tasks, while inferring locally on the constrained edge and mobile devices.},
  booktitle = {Proceedings of the 30th Annual International Conference on Mobile Computing and Networking},
  pages = {2236–2238},
  numpages = {3},
  location = {Washington D.C., DC, USA},
  series = {ACM MobiCom '24},
  poster={https://dl.acm.org/doi/10.1145/3636534.3697466},
  bibtex_show = {true},
  selected={true},
}

@ARTICLE{journal_rtl,
  author={Viswanadh Kandala, Savitha and Gureja, Akshit and Walchatwar, Nagesh and Agrawal, Rishabh and Sinha, Shiven and Chaudhari, Sachin and Vaidhyanathan, Karthik and Choppella, Venkatesh and Bhimalapuram, Prabhakar and Kandath, Harikumar and Hussain, Aftab},
  journal={IEEE Access}, 
  title={Engineering End-to-End Remote Labs Using IoT-Based Retrofitting}, 
  year={2025},
  volume={13},
  number={},
  pages={1106-1132},
  doi={10.1109/ACCESS.2024.3523066},
  bibtex_show = {true},
  selected={true},
  blog={https://blogs.iiit.ac.in/rtl/}
}

@INPROCEEDINGS{cv_rtl,
  bibtex_show = {true},
  author={Viswanadh, K. S. and Kathalkar, O. and Vinzey, P. and Nilesh, N. and Chaudhari, S. and Choppella, V.},
  booktitle={2022 9th International Conference on Future Internet of Things and Cloud (FiCloud)}, 
  title={CV and IoT-based Remote Triggered Labs: Use Case of Conservation of Mechanical Energy}, 
  year={2022},
  volume={},
  number={},
  pages={100-106},
  doi={10.1109/FiCloud57274.2022.00021}
  }

@INPROCEEDINGS{animesh_rtl,
  bibtex_show = {true},
  author={Das, A. and Viswanadh, K. S. and Agrawal, R. and Gureja, A. and Nilesh, N. and Chaudhari, S.},
  booktitle={2023 10th International Conference on Future Internet of Things and Cloud (FiCloud)}, 
  title={Using Miniature Setups and Partial Streams for Scalable Remote Labs}, 
  year={2023},
  volume={},
  number={},
  pages={256-263},
  doi={10.1109/FiCloud58648.2023.00045}
  }

@INPROCEEDINGS{ihita,
  bibtex_show = {true},
  author={Ihita, G.V. and Viswanadh, K.S. and Sudhansh, Y. and Chaudhari, S. and Gaur, S.},
  booktitle={2021 IEEE 7th World Forum on Internet of Things (WF-IoT)}, 
  title={Security Analysis of Large Scale IoT Network for Pollution Monitoring in Urban India}, 
  year={2021},
  volume={},
  number={},
  pages={283-288},
  doi={10.1109/WF-IoT51360.2021.9595688}
  }

@inproceedings{sigcomm_poster,
  bibtex_show = {true},
  author = {Medaranga, Pramuka and Shah, Dhairya and Kandala, Savitha Viswanadh and Varshney, Ambuj},
  title = {POSTER: Simplifying the Networking of Wireless Embedded Systems using a Large Language Model},
  year = {2024},
  isbn = {9798400707179},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3672202.3673752},
  doi = {10.1145/3672202.3673752},
  booktitle = {Proceedings of the ACM SIGCOMM 2024 Conference: Posters and Demos},
  pages = {78–80},
  numpages = {3},
  location = {Sydney, NSW, Australia},
  series = {ACM SIGCOMM Posters and Demos '24}
}


